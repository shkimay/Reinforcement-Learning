{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-LFyUlSB6sRK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FrozenLake_Slippery:\n",
        "  def __init__(self):\n",
        "    #Initialize the model (2 pts)\n",
        "    self.height = 4\n",
        "    self.width = 8\n",
        "    self.state_space = [(x,y) for x in range(self.height) for y in range(self.width)]         #Define state space\n",
        "    self.action_space = [0,1,2,3]        #Define action space (0: Up, 1: Down, 2: Left, 3: Right)\n",
        "    self.start_state = (0,0)           #Define start point\n",
        "    self.goal_state = (3,7)            #Define goal point\n",
        "    self.holes = [(0,4),(1,5),(2,1),(2,7),(3,2)]               #Define holes\n",
        "    self.slippery = 0.15\n",
        "    self.state = self.start_state\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = self.start_state\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    #Define reward-state transition (6 pts)\n",
        "    x = self.state[0]\n",
        "    y = self.state[1]\n",
        "\n",
        "    #true_action: actual movement\n",
        "    if np.random.rand() > self.slippery:\n",
        "      true_action = action\n",
        "    else:\n",
        "      actions = [a for a in [0, 1, 2, 3] if a != action]\n",
        "      true_action = np.random.choice(actions)\n",
        "\n",
        "\n",
        "    if true_action == 0:\n",
        "      x = max(x-1,0)     #state transition after choosing Up\n",
        "\n",
        "\n",
        "    elif true_action == 1:\n",
        "      x = min(x+1,3)       #state transition after choosing Down\n",
        "\n",
        "\n",
        "    elif true_action == 2:\n",
        "      y = max(y-1,0)      #state transition after choosing Left\n",
        "\n",
        "\n",
        "    elif true_action == 3:\n",
        "      y = min(y+1,7)        #state transition after choosing Right\n",
        "\n",
        "\n",
        "\n",
        "    self.state = (x, y)\n",
        "\n",
        "    if self.state in self.holes:\n",
        "      return self.state, -2, True                      #(terminal state, reward, done)\n",
        "\n",
        "    elif self.state == self.goal_state:\n",
        "      return self.state, 2, True                     #(terminal state, reward, done)\n",
        "\n",
        "    else:\n",
        "      return self.state, 0, False                     #(non-terminal state, reward, done)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def equiprobable_random_policy(state):\n",
        "  return np.random.choice([0, 1, 2, 3])"
      ],
      "metadata": {
        "id": "ZCoL390i7mAn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate 10 episodes (2 pts) - Do not change, just run\n",
        "np.random.seed(1)\n",
        "env = FrozenLake_Slippery()\n",
        "\n",
        "i = 1\n",
        "while i <= 10:\n",
        "  episode = []\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while (not done):\n",
        "    action = equiprobable_random_policy(state)\n",
        "    next_state, reward, done = env.step(action)\n",
        "    episode.append((state, action, reward))\n",
        "    state = next_state\n",
        "  episode.append((state, 'Terminal'))\n",
        "  print('Episode', i, ':', episode)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "lZgtAsAR66DT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fbe535-4a26-426e-c491-3e8df1048094"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 : [((0, 0), 1, 0), ((1, 0), 0, 0), ((1, 0), 3, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n",
            "Episode 2 : [((0, 0), 1, 0), ((1, 0), 1, 0), ((2, 0), 2, 0), ((2, 0), 0, 0), ((1, 0), 2, 0), ((1, 0), 2, 0), ((1, 0), 3, 0), ((1, 1), 1, 0), ((0, 1), 2, 0), ((0, 0), 1, 0), ((1, 0), 1, 0), ((2, 0), 1, 0), ((3, 0), 1, 0), ((3, 0), 3, 0), ((3, 1), 1, 0), ((3, 1), 3, -2), ((3, 2), 'Terminal')]\n",
            "Episode 3 : [((0, 0), 1, 0), ((1, 0), 3, 0), ((1, 1), 1, 0), ((1, 2), 0, 0), ((0, 2), 3, 0), ((0, 3), 3, -2), ((0, 4), 'Terminal')]\n",
            "Episode 4 : [((0, 0), 1, 0), ((1, 0), 3, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n",
            "Episode 5 : [((0, 0), 1, 0), ((0, 1), 3, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n",
            "Episode 6 : [((0, 0), 0, 0), ((0, 0), 2, 0), ((0, 0), 3, 0), ((0, 1), 3, 0), ((0, 0), 2, 0), ((0, 0), 0, 0), ((0, 0), 1, 0), ((1, 0), 3, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n",
            "Episode 7 : [((0, 0), 0, 0), ((0, 1), 0, 0), ((0, 0), 0, 0), ((0, 0), 3, 0), ((0, 1), 0, 0), ((0, 1), 1, 0), ((1, 1), 0, 0), ((0, 1), 0, 0), ((0, 1), 3, 0), ((0, 2), 3, 0), ((0, 3), 2, 0), ((0, 2), 0, 0), ((0, 2), 2, 0), ((0, 1), 0, 0), ((0, 1), 1, 0), ((0, 1), 3, 0), ((0, 0), 0, 0), ((0, 0), 2, 0), ((0, 0), 3, 0), ((0, 1), 2, 0), ((1, 1), 2, 0), ((1, 0), 0, 0), ((1, 0), 1, 0), ((2, 0), 1, 0), ((3, 0), 0, 0), ((2, 0), 0, 0), ((1, 0), 3, 0), ((2, 0), 1, 0), ((3, 0), 0, 0), ((2, 0), 2, 0), ((2, 0), 3, -2), ((2, 1), 'Terminal')]\n",
            "Episode 8 : [((0, 0), 3, 0), ((0, 1), 3, 0), ((0, 2), 0, 0), ((0, 1), 0, 0), ((0, 1), 1, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n",
            "Episode 9 : [((0, 0), 1, 0), ((1, 0), 0, 0), ((0, 0), 3, 0), ((0, 1), 2, 0), ((0, 0), 1, 0), ((1, 0), 3, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n",
            "Episode 10 : [((0, 0), 3, 0), ((0, 1), 0, 0), ((0, 1), 3, 0), ((0, 1), 2, 0), ((1, 1), 3, 0), ((0, 1), 1, 0), ((1, 1), 1, -2), ((2, 1), 'Terminal')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose SARSA or QLearning (If you don't use SARSA, delete this code)\n",
        "class QLearning:\n",
        "  def __init__(self, env, gamma = 0.9, epsilon = 0.1, alpha = 0.2):  #You should determine epsilon and alpha by yourself\n",
        "    #Initialize (5 pts)\n",
        "    self.env = env\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.alpha = alpha\n",
        "    self.q_table = {state: np.zeros(len(self.env.action_space)) for state in self.env.state_space}\n",
        "\n",
        "  def epsilon_greedy(self, state):\n",
        "      if np.random.rand() < self.epsilon:\n",
        "        return np.random.choice(self.env.action_space)\n",
        "      else:\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "  def control(self, episodes = 200):  #You should determine the number of episodes by yourself\n",
        "    for i in range(1, episodes+1):\n",
        "      if i % int(episodes/10) == 0:\n",
        "        print('Episode ', i, '/', episodes, ': ', '[', '*'*int(i/(episodes/10)), '-'*int((episodes - i)/(episodes/10)), ']')\n",
        "\n",
        "      #Initialize (do not change this code)\n",
        "      state = self.env.reset()\n",
        "      done = False\n",
        "\n",
        "      while (not done):\n",
        "        action = self.epsilon_greedy(state)\n",
        "        next_state, reward, done = self.env.step(action)\n",
        "        self.q_table[state][action] += self.alpha*(reward + self.gamma*max(self.q_table[next_state]) - self.q_table[state][action])\n",
        "        state = next_state\n",
        "\n",
        "    policy = {state: np.argmax(actions) for state, actions in self.q_table.items()}\n",
        "    return policy, self.q_table"
      ],
      "metadata": {
        "id": "NN8IvA818Jru"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class double_Qlearning:\n",
        "  def __init__(self, env, gamma = 0.9, epsilon = 0.1, alpha = 0.2):  #You should determine epsilon and alpha by yourself\n",
        "    #Initialize (5 pts)\n",
        "    self.env = env\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.alpha = alpha\n",
        "    self.q_table_action = {state: np.zeros(len(self.env.action_space)) for state in self.env.state_space}\n",
        "    self.q_table_value = {state: np.zeros(len(self.env.action_space)) for state in self.env.state_space}\n",
        "\n",
        "  def epsilon_greedy(self, state):\n",
        "      if np.random.rand() < self.epsilon:\n",
        "        return np.random.choice(self.env.action_space)\n",
        "      else:\n",
        "        return np.argmax(self.q_table_action[state])\n",
        "\n",
        "  def control(self, episodes = 200):  #You should determine the number of episodes by yourself\n",
        "    for i in range(1, episodes+1):\n",
        "      if i % int(episodes/10) == 0:\n",
        "        print('DQL Episode ', i, '/', episodes, ': ', '[', '*'*int(i/(episodes/10)), '-'*int((episodes - i)/(episodes/10)), ']')\n",
        "\n",
        "      #Initialize (do not change this code)\n",
        "      state = self.env.reset()\n",
        "      done = False\n",
        "\n",
        "      while (not done):\n",
        "        action = self.epsilon_greedy(state)\n",
        "        next_state, reward, done = self.env.step(action)\n",
        "        A = np.argmax(self.q_table_action[next_state])\n",
        "        self.q_table_action[state][action] += self.alpha*(reward + self.gamma*(self.q_table_value[next_state][A]) - self.q_table_action[state][action])\n",
        "        state = next_state\n",
        "\n",
        "    policy = {state: np.argmax(actions) for state, actions in self.q_table_action.items()}\n",
        "    return policy, self.q_table_action"
      ],
      "metadata": {
        "id": "8gE0oXzMmK2Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = FrozenLake_Slippery()\n",
        "\n",
        "#Determine the hyperparameters by yourself\n",
        "QL_FrozenLake = QLearning(env, gamma = 0.9, epsilon = 0.1, alpha =0.2)\n",
        "DQL_FrozenLake = double_Qlearning(env, gamma = 0.9, epsilon = 0.1, alpha =0.2)\n",
        "opt_policy, q_table = QL_FrozenLake.control(episodes = 200)\n",
        "DQL_opt_policy, q_table_action = DQL_FrozenLake.control(episodes = 200)"
      ],
      "metadata": {
        "id": "NFf4qlo8_8_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6a73b5-8f37-45e6-e262-db822e142b86"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  20 / 200 :  [ * --------- ]\n",
            "Episode  40 / 200 :  [ ** -------- ]\n",
            "Episode  60 / 200 :  [ *** ------- ]\n",
            "Episode  80 / 200 :  [ **** ------ ]\n",
            "Episode  100 / 200 :  [ ***** ----- ]\n",
            "Episode  120 / 200 :  [ ****** ---- ]\n",
            "Episode  140 / 200 :  [ ******* --- ]\n",
            "Episode  160 / 200 :  [ ******** -- ]\n",
            "Episode  180 / 200 :  [ ********* - ]\n",
            "Episode  200 / 200 :  [ **********  ]\n",
            "DQL Episode  20 / 200 :  [ * --------- ]\n",
            "DQL Episode  40 / 200 :  [ ** -------- ]\n",
            "DQL Episode  60 / 200 :  [ *** ------- ]\n",
            "DQL Episode  80 / 200 :  [ **** ------ ]\n",
            "DQL Episode  100 / 200 :  [ ***** ----- ]\n",
            "DQL Episode  120 / 200 :  [ ****** ---- ]\n",
            "DQL Episode  140 / 200 :  [ ******* --- ]\n",
            "DQL Episode  160 / 200 :  [ ******** -- ]\n",
            "DQL Episode  180 / 200 :  [ ********* - ]\n",
            "DQL Episode  200 / 200 :  [ **********  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the optimal policy (10 pts)\n",
        "policy = [[0 for _ in range(8)] for _ in range(4)]\n",
        "\n",
        "for state, action in opt_policy.items():\n",
        "  if action == 0:\n",
        "    policy[state[0]][state[1]] = '^'\n",
        "  elif action == 1:\n",
        "    policy[state[0]][state[1]] = 'v'\n",
        "  elif action == 2:\n",
        "    policy[state[0]][state[1]] = '<'\n",
        "  elif action == 3:\n",
        "    policy[state[0]][state[1]] = '>'\n",
        "\n",
        "policy[0][4], policy[1][5], policy[2][1], policy[2][7], policy[3][2] = 'H', 'H', 'H', 'H', 'H'\n",
        "policy[3][7] = 'G'\n",
        "policy"
      ],
      "metadata": {
        "id": "3JzfNjkIAKy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7975f871-3321-46ab-b0ba-abb852811b0f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['>', '>', '>', 'v', 'H', '>', '>', '^'],\n",
              " ['^', '<', '^', 'v', 'v', 'H', '>', '^'],\n",
              " ['^', 'H', '^', '>', 'v', '>', 'v', 'H'],\n",
              " ['<', '<', 'H', '>', '>', '>', '>', 'G']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DQL_policy = [[0 for _ in range(8)] for _ in range(4)]\n",
        "\n",
        "for state, action in DQL_opt_policy.items():\n",
        "  if action == 0:\n",
        "    DQL_policy[state[0]][state[1]] = '^'\n",
        "  elif action == 1:\n",
        "    DQL_policy[state[0]][state[1]] = 'v'\n",
        "  elif action == 2:\n",
        "    DQL_policy[state[0]][state[1]] = '<'\n",
        "  elif action == 3:\n",
        "    DQL_policy[state[0]][state[1]] = '>'\n",
        "\n",
        "DQL_policy[0][4], DQL_policy[1][5], DQL_policy[2][1], DQL_policy[2][7], DQL_policy[3][2] = 'H', 'H', 'H', 'H', 'H'\n",
        "DQL_policy[3][7] = 'G'\n",
        "DQL_policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLXttUNOfEA2",
        "outputId": "fb4f7710-5dc6-4989-9b15-b81f693564f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['^', '^', '^', '^', 'H', '^', '^', '^'],\n",
              " ['^', '>', '^', '^', 'v', 'H', '^', '^'],\n",
              " ['v', 'H', '^', '^', '^', '<', '^', 'H'],\n",
              " ['^', 'v', 'H', '^', '^', '^', '^', 'G']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}