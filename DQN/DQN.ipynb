{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SQB5-0nKarT",
        "outputId": "0a912a00-9db9-409d-dbdc-ab23d531ca2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MachineEnv:\n",
        "  def __init__(self):\n",
        "    self.state = np.zeros(3)\n",
        "    self.action = np.array([0, 1, 2, 3])\n",
        "    self.max_time = 100.\n",
        "    self.maintenance_cost = 10.\n",
        "    self.breakdown_cost = 100.\n",
        "    self.profit = 20.\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = np.zeros(3)\n",
        "    return self.state\n",
        "\n",
        "  def failure_prob(self):\n",
        "    return 1 - np.exp(-0.3*(self.state[0] + self.state[1]) - 0.1*self.state[2]/self.max_time)\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == 0:\n",
        "      if np.random.rand() < self.failure_prob():\n",
        "        reward = -self.breakdown_cost\n",
        "        self.state[0] = 0\n",
        "        self.state[1] = 0\n",
        "      else:\n",
        "        reward = self.profit\n",
        "        self.state[0] += np.random.rand()*0.1\n",
        "        self.state[1] += np.random.rand()*0.05\n",
        "\n",
        "    elif action == 1:\n",
        "      reward = -self.maintenance_cost\n",
        "      self.state[0] = 0.\n",
        "\n",
        "    elif action == 2:\n",
        "      reward = -self.maintenance_cost\n",
        "      self.state[1] = 0.\n",
        "\n",
        "    elif action == 3:\n",
        "      reward = -2*self.maintenance_cost\n",
        "      self.state[0] = 0.\n",
        "      self.state[1] = 0.\n",
        "\n",
        "    self.state[0] = min(1, self.state[0])\n",
        "    self.state[1] = min(1, self.state[1])\n",
        "    self.state[2] += 1.\n",
        "\n",
        "    if self.state[2] == self.max_time:\n",
        "      return self.state, reward, True\n",
        "    else:\n",
        "      return self.state, reward, False"
      ],
      "metadata": {
        "id": "slbwbjaPKlQq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, hidden_dim_1 = 64, hidden_dim_2 = 64):\n",
        "    super().__init__()\n",
        "    self.net1 = nn.Linear(state_dim, hidden_dim_1)\n",
        "    self.net2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "    self.net3 = nn.Linear(hidden_dim_2, action_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, s):\n",
        "    x = self.relu(self.net1(s))\n",
        "    x = self.relu(self.net2(x))\n",
        "    return self.net3(x)"
      ],
      "metadata": {
        "id": "NzmmP9NyKoJa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, state_dim, action_dim, max_size = 10000):\n",
        "    self.max_size = max_size\n",
        "    self.count = 0\n",
        "    self.size = 0\n",
        "    self.s = np.zeros(shape = (self.max_size, state_dim))\n",
        "    self.a = np.zeros(shape = (self.max_size, 1))\n",
        "    self.r = np.zeros(shape = (self.max_size, 1))\n",
        "    self.s_next = np.zeros(shape = (self.max_size, state_dim))\n",
        "    self.done = np.zeros(shape = (self.max_size, 1))\n",
        "\n",
        "  def store(self, s, a, r, s_next, done):\n",
        "    self.s[self.count] = s\n",
        "    self.a[self.count] = a\n",
        "    self.r[self.count] = r\n",
        "    self.s_next[self.count] = s_next\n",
        "    self.done[self.count] = done\n",
        "\n",
        "    self.count = (self.count+1) % self.max_size\n",
        "    self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    idx = np.random.choice(self.size, size = batch_size)\n",
        "    batch_s = torch.tensor(self.s[idx], dtype=torch.float32).to(device)\n",
        "    batch_a = torch.tensor(self.a[idx], dtype=torch.long).to(device)\n",
        "    batch_r = torch.tensor(self.r[idx], dtype=torch.float32).to(device)\n",
        "    batch_s_next = torch.tensor(self.s_next[idx], dtype=torch.float32).to(device)\n",
        "    batch_done = torch.tensor(self.done[idx], dtype=torch.float32).to(device)\n",
        "    return batch_s, batch_a, batch_r, batch_s_next, batch_done"
      ],
      "metadata": {
        "id": "d2PXOAlpKqwC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN_Learning:\n",
        "  def __init__(self, state_dim, action_dim, alpha, gamma, tau, K):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "\n",
        "    self.q = DQN(state_dim, action_dim).to(device)\n",
        "    self.q_target = DQN(state_dim, action_dim).to(device)\n",
        "    self.optimizer = optim.Adam(self.q.parameters(), lr = alpha)\n",
        "    self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    self.memory = ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "    self.t_step = 0\n",
        "    self.K = K\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "\n",
        "  def step(self, s, a, r, s_next, done):\n",
        "    self.memory.store(s, a, r, s_next, done)\n",
        "\n",
        "    self.t_step = (self.t_step + 1) % self.K\n",
        "    if self.t_step == 0:\n",
        "      if self.memory.size >= 64:\n",
        "        batch_sample = self.memory.sample(64)\n",
        "        self.learn(batch_sample)\n",
        "\n",
        "  def choose_action(self, state, epsilon):\n",
        "    state_tensor = torch.tensor(state, dtype = torch.float32).reshape(1, -1).to(device)\n",
        "\n",
        "    self.q.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.q(state_tensor)\n",
        "    self.q.train()\n",
        "\n",
        "    if np.random.rand() > epsilon:\n",
        "        return action_values.argmax(dim=1).item()\n",
        "    else:\n",
        "        return np.random.randint(self.action_dim)\n",
        "\n",
        "  def learn(self, batch_sample):\n",
        "    s, a, r, s_next, done = batch_sample\n",
        "\n",
        "    q_targets_next = self.q_target(s_next).detach().max(dim=1)[0].reshape(-1, 1)\n",
        "    q_targets = r + (self.gamma*q_targets_next*(1-done))\n",
        "    q_local = self.q(s).gather(dim=1, index=a)\n",
        "\n",
        "    loss = self.loss_fn(q_local, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self.soft_update(self.q, self.q_target)\n",
        "\n",
        "  def soft_update(self, q, q_target):\n",
        "    for target_param, local_param in zip(q_target.parameters(), q.parameters()):\n",
        "      target_param.data = self.tau*local_param.data + (1.0 - self.tau)*target_param.data"
      ],
      "metadata": {
        "id": "GzwoFDrZKvIa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = MachineEnv()\n",
        "\n",
        "num_episode = 300\n",
        "max_steps = 100\n",
        "epsilon_start = 0.4\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay_rate = 0.9\n",
        "gamma = 1\n",
        "K = 64\n",
        "tau = 0.005\n",
        "lr = 0.001\n",
        "freq = 50\n",
        "\n",
        "DQN_learner = DQN_Learning(len(env.state), len(env.action), lr, gamma, tau, K)\n",
        "def DQN_policy(num_episodes,freq,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps):\n",
        "  total_reward = 0\n",
        "  training_rewards = 0\n",
        "  for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
        "    for step in range(max_steps):\n",
        "      action = DQN_learner.choose_action(state, epsilon)\n",
        "      next_state, reward, done = env.step(action)\n",
        "      DQN_learner.step(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      episode_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    total_reward += episode_reward\n",
        "    training_rewards += episode_reward\n",
        "    if (episode + 1) % freq == 0:\n",
        "      print(f\"Episode {episode + 1}: Average rewards for the last {freq} episodes: {training_rewards / freq:.2f}\")\n",
        "      training_rewards = 0\n",
        "  avg_reward = total_reward/num_episodes\n",
        "  return avg_reward\n",
        "DQN_policy(num_episode,freq,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox3Yi7vAKyBq",
        "outputId": "0c5c711f-679a-4a2b-d503-da5d4420d39f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50: Average rewards for the last 50 episodes: 406.20\n",
            "Episode 100: Average rewards for the last 50 episodes: 339.80\n",
            "Episode 150: Average rewards for the last 50 episodes: 450.20\n",
            "Episode 200: Average rewards for the last 50 episodes: 410.60\n",
            "Episode 250: Average rewards for the last 50 episodes: 436.20\n",
            "Episode 300: Average rewards for the last 50 episodes: 363.60\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "401.1"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = MachineEnv()\n",
        "PM = np.zeros(shape=(10, 10))\n",
        "def heuristic1(PM,num_episodes,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps):\n",
        "  for i, ii in enumerate(range(1, 11)):\n",
        "    for j, jj in enumerate(range(1, 11)):\n",
        "      training_rewards = 0\n",
        "      for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
        "        episode_reward = 0\n",
        "        for step in range(max_steps):\n",
        "          if (step % ii == 0 and step % jj == 0):\n",
        "            action = 3\n",
        "          elif step % ii == 0:\n",
        "            action = 1\n",
        "          elif step % jj == 0:\n",
        "            action = 2\n",
        "          else:\n",
        "            action = DQN_learner.choose_action(state, epsilon)\n",
        "          next_state, reward, done = env.step(action)\n",
        "          DQN_learner.step(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          episode_reward += reward\n",
        "          if done:\n",
        "            break\n",
        "        training_rewards += episode_reward\n",
        "      PM[i][j] = training_rewards / num_episodes\n",
        "  best_i, best_j = np.unravel_index(np.argmax(PM), PM.shape)\n",
        "  best_reward = PM[best_i][best_j]\n",
        "  return best_reward, best_i+1, best_j+1\n",
        "best_reward, best_i, best_j = heuristic1(PM,100,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps)\n",
        "print(f\" reward: {best_reward}, part1: {best_i}, part2: {best_j}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGcvvMsiK476",
        "outputId": "c21a7b0a-1c40-4863-ecb2-0f65ffce5caf"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " reward: 178.4, part1: 3, part2: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = MachineEnv()\n",
        "CM = np.zeros(shape=(10, 10))\n",
        "max_steps = 100\n",
        "freq = 100\n",
        "epsilon_start = 0.3\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay_rate = 0.9\n",
        "def heuristic2(CM,num_episodes,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps):\n",
        "  for i, ii in enumerate(range(5, 51, 5)):\n",
        "    for j, jj in enumerate(range(5, 51, 5)):\n",
        "      training_rewards = 0\n",
        "      for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
        "        episode_reward = 0\n",
        "        for step in range(max_steps):\n",
        "          if (state[0] > (ii/100) and state[1] > (jj/100)):\n",
        "            action = 3\n",
        "          elif state[0] > (ii/100):\n",
        "            action = 1\n",
        "          elif state[1] > (jj/100):\n",
        "            action = 2\n",
        "          else:\n",
        "            action = DQN_learner.choose_action(state, epsilon)\n",
        "          next_state, reward, done = env.step(action)\n",
        "          DQN_learner.step(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          episode_reward += reward\n",
        "          if done:\n",
        "              break\n",
        "        training_rewards += episode_reward\n",
        "      CM[i-1][j-1] = training_rewards / num_episodes\n",
        "  Best_i, Best_j = np.unravel_index(np.argmax(CM), CM.shape)\n",
        "  best_reward = CM[Best_i][Best_j]\n",
        "  return best_reward, (Best_i+1)*0.05, (Best_j+1)*0.05\n",
        "best_reward2, best_i2, best_j2 = heuristic2(CM,100,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps)\n",
        "print(f\" reward: {best_reward2}, part1: {best_i2}, part2: {best_j2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2fK83LYMb6a",
        "outputId": "c36a8b5b-553c-42cb-e5dd-265123f16ab6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " reward: -629.7, part1: 0.30000000000000004, part2: 0.35000000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DQN_reward = DQN_policy(100,freq,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps)\n",
        "\n",
        "heuristic1_reward, best_i, best_j =  heuristic1(PM,100,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps)\n",
        "\n",
        "heuristic2_reward, best_i2, best_j2 = heuristic2(CM,100,epsilon_start,epsilon_end,epsilon_decay_rate,max_steps)\n",
        "\n",
        "print(f\"DQN reward: {DQN_reward}\")\n",
        "print(f\"heuristic 1 reward: {heuristic1_reward}\")\n",
        "print(f\"heuristic 2 reward: {heuristic2_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU8ThNYu3q6L",
        "outputId": "d7d097e3-369b-4417-e454-c9e90c6d5779"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100: Average rewards for the last 100 episodes: -1021.00\n",
            "DQN reward: -1021.0\n",
            "heuristic 1 reward: -223.1\n",
            "heuristic 2 reward: -521.6\n"
          ]
        }
      ]
    }
  ]
}