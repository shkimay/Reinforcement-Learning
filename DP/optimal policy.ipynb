{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8M0qiYPD3jKU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#define the transition (default size = 5x5)\n",
        "def next_state_reward(state, action, height = 5, width = 5):\n",
        "  action_move = [(-1, 0), (1, 0), (0, -1), (0, 1)] #4 possible actions -  0: Up / 1: Down / 2: Left / 3: Right\n",
        "\n",
        "  #state transition: states = (height, width)\n",
        "  state[0] += action_move[action][0]\n",
        "  state[1] += action_move[action][1]\n",
        "\n",
        "  #treating some blocked moves\n",
        "  if state[0] < 0:\n",
        "    state[0] = 0\n",
        "  elif state[0] >  height-1:\n",
        "    state[0] = height-1\n",
        "  if state[1] < 0:\n",
        "    state[1] = 0\n",
        "  elif state[1] > width-1:\n",
        "    state[1] = width-1\n",
        "\n",
        "  #return the new state according to the selected action and reward = -1\n",
        "  return [state[0], state[1]], -1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mysterious_state_reward(state, action, height = 5, width = 5):\n",
        "  action_move = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "  mys = []\n",
        "\n",
        "  mys_1 = [1,3]\n",
        "  mys_2 = [0,3]\n",
        "\n",
        "  if state in ([0,2] or [2,1]):\n",
        "    mys = mys_1\n",
        "  else:\n",
        "    mys = mys_2\n",
        "\n",
        "  a = random.choice(mys)\n",
        "  state[0] += action_move[a][0]\n",
        "  state[1] += action_move[a][1]\n",
        "\n",
        "  if state[0] < 0:\n",
        "    state[0] = 0\n",
        "  elif state[0] >  height-1:\n",
        "    state[0] = height-1\n",
        "  if state[1] < 0:\n",
        "    state[1] = 0\n",
        "  elif state[1] > width-1:\n",
        "    state[1] = width-1\n",
        "\n",
        "  return [state[0], state[1]], 2"
      ],
      "metadata": {
        "id": "pAyFSOsddOnW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Dv9ANP9o3xlV"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(action, policy, height = 5, width = 5, theta=1e-5, gamma=0.9):\n",
        "  #initialize state value function\n",
        "  value_table = np.zeros(shape=(height, width))\n",
        "\n",
        "  #two-array version (try in-place version by yourself)\n",
        "  iter = 0\n",
        "  while iter<2000:\n",
        "    #update the new state value function\n",
        "    next_value_table = np.zeros(shape=(height, width))\n",
        "    delta = 0\n",
        "    for i in range(height):\n",
        "      for j in range(width):\n",
        "\n",
        "        #always assign 0 to the terminal states\n",
        "        if ((i == 0) and (j == 0)) or ((i == height-1) and (j == height-1)):\n",
        "          value_iter = 0\n",
        "\n",
        "        else:\n",
        "          #update the non-terminal states using the Bellman equation\n",
        "          value_iter = 0\n",
        "\n",
        "          for act in action:\n",
        "            if [i,j] in [[0,2],[2,1],[3,3]]:\n",
        "                transition_prob = 0.5\n",
        "                next_s, r = mysterious_state_reward([i,j], act)\n",
        "\n",
        "            else:\n",
        "                transition_prob = 1 #deterministic\n",
        "                next_s, r = next_state_reward([i,j], act)\n",
        "            value_iter += policy[i][j][act]*transition_prob*(r + gamma*value_table[next_s[0]][next_s[1]])\n",
        "\n",
        "          #deterministic state transition (do not consider the transition probability in this case)\n",
        "          # for act in action:\n",
        "          #   next_s, r = next_state_reward([i,j], act)\n",
        "          #   value_iter += policy[i][j][act]*transition_prob*(r + gamma*value_table[next_s[0]][next_s[1]])\n",
        "\n",
        "        #update the state value function\n",
        "        next_value_table[i][j] = round(value_iter, 3)\n",
        "\n",
        "        #compare the error and the error bound\n",
        "        delta = max(delta, abs(next_value_table[i][j] - value_table[i][j]))\n",
        "\n",
        "    value_table = next_value_table\n",
        "    iter += 1\n",
        "\n",
        "    #termination condition\n",
        "    if delta < theta:\n",
        "      # print('Final results ({} iterations): \\n {}'.format(iter, next_value_table))\n",
        "      break\n",
        "\n",
        "    #print the results\n",
        "    # iter_visual = [1, 2, 10, 50] + [n*100 for n in range(20)]\n",
        "    # if iter in iter_visual:\n",
        "    #   print('iteration {}: \\n {}'.format(iter, next_value_table))\n",
        "\n",
        "  return next_value_table"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "policy_evaluation in-place update"
      ],
      "metadata": {
        "id": "1nghbC1lvVXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation_in_place(action, policy, height=5, width=5, theta=1e-5, gamma=0.9):\n",
        "    # Initialize state value function\n",
        "    value_table = np.zeros(shape=(height, width))\n",
        "\n",
        "    iter = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "\n",
        "        for i in range(height):\n",
        "            for j in range(width):\n",
        "\n",
        "                # Always assign 0 to the terminal states\n",
        "                if ((i == 0) and (j == 0)) or ((i == height-1) and (j == width-1)):\n",
        "                    continue\n",
        "\n",
        "                old_value = value_table[i][j]\n",
        "\n",
        "                # Update non-terminal states using the Bellman equation\n",
        "                value_iter = 0\n",
        "                for act in action:\n",
        "                    if [i,j] in [[0,2],[2,1],[3,3]]:\n",
        "                      transition_prob = 0.5\n",
        "                      next_s, r = mysterious_state_reward([i,j], act)\n",
        "\n",
        "                    else:\n",
        "                      transition_prob = 1 #deterministic\n",
        "                      next_s, r = next_state_reward([i,j], act)\n",
        "                    value_iter += policy[i][j][act]*transition_prob*(r + gamma*value_table[next_s[0]][next_s[1]])\n",
        "\n",
        "                value_table[i][j] = round(value_iter, 3)\n",
        "\n",
        "                # Compare the error and update delta\n",
        "                delta = max(delta, abs(old_value - value_table[i][j]))\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "        # iter_visual = [1, 2, 10, 50] + [n*100 for n in range(20)]\n",
        "        # if iter in iter_visual:\n",
        "        #     print('iteration {}: \\n {}'.format(iter, value_table))\n",
        "\n",
        "    return value_table\n"
      ],
      "metadata": {
        "id": "JJUibs6lvRH4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ1Ib4x7-QJt",
        "outputId": "034a96a9-410a-4560-9eff-0938d090243b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vㅠ(s): [[ 0.    -2.182  0.028 -4.592 -6.297]\n",
            " [-3.48  -3.355 -3.762 -5.334 -6.198]\n",
            " [-4.212 -0.729 -4.    -4.404 -5.239]\n",
            " [-5.86  -4.828 -4.276 -0.338 -2.97 ]\n",
            " [-6.725 -6.152 -5.18  -2.955  0.   ]]\n"
          ]
        }
      ],
      "source": [
        "#grid environment\n",
        "grid_height, grid_width = 5, 5 #5x5 gridworld\n",
        "action = [0, 1, 2, 3] #0: up / 1: down / 2: left / 3: right\n",
        "\n",
        "#policy initialization\n",
        "policy = np.zeros(shape=(grid_height, grid_width, len(action)))\n",
        "\n",
        "#random equiprobable policy\n",
        "for i in range(grid_height):\n",
        "  for j in range(grid_width):\n",
        "    for k in range(len(action)):\n",
        "      if ((i == 0) and (j == 0)) or ((i == grid_height-1) and (j == grid_height-1)):\n",
        "        policy[i][j][k] = 0\n",
        "      else:\n",
        "        policy[i][j][k] = 0.25\n",
        "\n",
        "\n",
        "state_value = policy_evaluation(action, policy)\n",
        "# in_place_state_value = policy_evaluation_in_place(action, policy)\n",
        "print('Vㅠ(s):',state_value)\n",
        "# print('V(s):',in_place_state_value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(value, action, policy, height = 5, width = 5, gamma = 0.9):\n",
        "  #improved policy\n",
        "  new_policy = np.zeros(shape=(height, width, len(action)))\n",
        "\n",
        "  for i in range(height):\n",
        "    for j in range(width):\n",
        "      if ((i == 0) and (j == 0)) or ((i == height-1) and (j == height-1)):\n",
        "        pass\n",
        "\n",
        "      else:\n",
        "        #compute q(s, a) for every state\n",
        "        q_list = []\n",
        "\n",
        "        for k in action:\n",
        "          if [i,j] in [[0,2],[2,1],[3,3]]:\n",
        "              transition_prob = 0.5\n",
        "              next_s, r = mysterious_state_reward([i,j], k)\n",
        "\n",
        "          else:\n",
        "              transition_prob = 1 #deterministic\n",
        "              next_s, r = next_state_reward([i,j], k)\n",
        "          q_list.append(transition_prob*(r + gamma*value[next_s[0]][next_s[1]]))\n",
        "\n",
        "        #find the argmax\n",
        "        max_actions = [act for act, x in enumerate(q_list) if x == max(q_list)]\n",
        "\n",
        "        #update the policy (new equiprobable)\n",
        "        for k in action:\n",
        "          if k in max_actions:\n",
        "            new_policy[i][j][k] = 1/len(max_actions)\n",
        "\n",
        "  print('Updated policy is {}'.format(new_policy))\n",
        "  return new_policy"
      ],
      "metadata": {
        "id": "s_QwvK_zmw7v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improved_policy = policy_improvement(state_value, action, policy)\n",
        "print(improved_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUA9SPW0o-IB",
        "outputId": "67117177-beed-4bcd-ac91-7a90e228c987"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated policy is [[[0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.5        0.         0.5        0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[0.         0.         0.         1.        ]\n",
            "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.33333333 0.         0.33333333 0.33333333]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         0.        ]]]\n",
            "[[[0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.5        0.         0.5        0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[0.         0.         0.         1.        ]\n",
            "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.33333333 0.         0.33333333 0.33333333]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(action, init_policy, height = 5, width = 5):\n",
        "  #initial policy\n",
        "  policy = init_policy\n",
        "\n",
        "  while True:\n",
        "    #false if updated policy is not equal to original policy\n",
        "    policy_stable = True\n",
        "\n",
        "    #policy evaluation & improveemnt\n",
        "    state_value = policy_evaluation(action, policy)\n",
        "    # in_place_state_value = policy_evaluation_in_place(action, policy)\n",
        "    improved_policy = policy_improvement(state_value, action, policy)\n",
        "    # improved_policy_in_place = policy_improvement(in_place_state_value, action, policy)\n",
        "\n",
        "    #check whether the two policies are identical\n",
        "    for i in range(height):\n",
        "      for j in range(width):\n",
        "        for k in range(len(action)):\n",
        "         if policy[i][j][k] != improved_policy[i][j][k]:\n",
        "          policy_stable = False\n",
        "\n",
        "    #update the policy\n",
        "    policy = improved_policy\n",
        "    # policy = improved_policy_in_place\n",
        "\n",
        "    #termination condition\n",
        "    if policy_stable:\n",
        "      break\n",
        "\n",
        "  return policy\n"
      ],
      "metadata": {
        "id": "07qKeHatm3cc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(policy_iteration(action, policy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABWVVq4Sm6q8",
        "outputId": "987e0012-153a-44fc-ed79-875c175ec44b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated policy is [[[0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[0.         0.         0.         1.        ]\n",
            "  [0.33333333 0.         0.33333333 0.33333333]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.5        0.         0.         0.5       ]\n",
            "  [0.         1.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.         0.         0.        ]]]\n",
            "Updated policy is [[[0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         0.33333333 0.33333333 0.33333333]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.5        0.         0.5        0.        ]\n",
            "  [0.5        0.         0.5        0.        ]]\n",
            "\n",
            " [[0.         0.         0.         1.        ]\n",
            "  [0.25       0.25       0.25       0.25      ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[0.5        0.         0.         0.5       ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[0.5        0.         0.         0.5       ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.        ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.5  0.5  0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.5  0.   0.5 ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.5  0.   0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.5        0.         0.5        0.        ]\n",
            "  [0.5        0.         0.5        0.        ]]\n",
            "\n",
            " [[0.         0.         0.         1.        ]\n",
            "  [0.25       0.25       0.25       0.25      ]\n",
            "  [0.         0.         1.         0.        ]\n",
            "  [0.         1.         0.         0.        ]\n",
            "  [0.         0.5        0.5        0.        ]]\n",
            "\n",
            " [[0.5        0.         0.         0.5       ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         1.        ]\n",
            "  [0.25       0.25       0.25       0.25      ]\n",
            "  [0.         0.         1.         0.        ]]\n",
            "\n",
            " [[0.5        0.         0.         0.5       ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.5        0.         0.         0.5       ]\n",
            "  [1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.        ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.5  0.   0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "Updated policy is [[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.5  0.   0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n",
            "[[[0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.5  0.   0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[1.   0.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.5  0.  ]\n",
            "  [0.5  0.   0.5  0.  ]]\n",
            "\n",
            " [[0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   0.5  0.5  0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   1.   0.  ]]\n",
            "\n",
            " [[0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.5  0.   0.   0.5 ]\n",
            "  [1.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.  ]]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}